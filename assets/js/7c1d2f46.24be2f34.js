"use strict";(self.webpackChunkupb_rustworkshop_github_io=self.webpackChunkupb_rustworkshop_github_io||[]).push([[8476],{2785:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>h,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>a});const l=JSON.parse('{"id":"edge_ai/chat_with_llm/index","title":"Chat With LLM","description":"The Chat with LLM workshop will guide you through four essential techniques used for interacting with LLMs:","source":"@site/docs/edge_ai/chat_with_llm/index.md","sourceDirName":"edge_ai/chat_with_llm","slug":"/edge_ai/chat_with_llm/","permalink":"/docs/edge_ai/chat_with_llm/","draft":false,"unlisted":false,"editUrl":"https://github.com/UPB-RustWorkshop/upb-rustworkshop.github.io/edit/main/docs/edge_ai/chat_with_llm/index.md","tags":[],"version":"current","frontMatter":{"position":3},"sidebar":"tutorialSidebar","previous":{"title":"Third-Party Libraries Cheatsheet for Face Auth Workshop","permalink":"/docs/edge_ai/face_authentication/cheatsheet"},"next":{"title":"Exercise 01: Simple Chat Request","permalink":"/docs/edge_ai/chat_with_llm/chat_request"}}');var s=t(4848),n=t(8453);const r={position:3},o="Chat With LLM",h={},a=[{value:"Slides",id:"slides",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploying the models",id:"deploying-the-models",level:3},{value:"Repository",id:"repository",level:2},{value:"Workshop",id:"workshop",level:2}];function d(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,n.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"chat-with-llm",children:"Chat With LLM"})}),"\n",(0,s.jsx)(i.p,{children:"The Chat with LLM workshop will guide you through four essential techniques used for interacting with LLMs:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Simple chat request"}),"\n",(0,s.jsx)(i.li,{children:"RAG"}),"\n",(0,s.jsx)(i.li,{children:"Structured outputs"}),"\n",(0,s.jsx)(i.li,{children:"Tool calling"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:'The application runs in the CLI and expects a user prompt. The user then selects one of the available techniques to interact with the LLM. The model will respond. The messages inside the conversation are stored in memory. The application will keep running until the user types "exit".'}),"\n",(0,s.jsx)(i.h2,{id:"slides",children:"Slides"}),"\n",(0,s.jsx)("iframe",{src:"/pdf/edgeai/2_llm_rustworkshop.pdf",loading:"lazy",width:"700",height:"400",children:(0,s.jsx)(i.p,{children:"Not able to display the slides"})}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)("a",{href:"/pdf/edgeai/2_llm_rustworkshop.pdf",target:"_blank",children:"download the slides"}),"."]}),"\n",(0,s.jsx)(i.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsx)(i.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(i.p,{children:"The following are already installed on the Raspberry Pi:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.a,{href:"https://doc.rust-lang.org/cargo/getting-started/installation.html",children:"Cargo"})}),"\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.a,{href:"https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cpu-build",children:"Llama.cpp"})}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"deploying-the-models",children:"Deploying the models"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"llama-server --embeddings --hf-repo second-state/All-MiniLM-L6-v2-Embedding-GGUF --hf-file  all-MiniLM-L6-v2-ggml-model-f16.gguf --port 8081 # embeddings model available on localhost:8081\nllama-server --jinja --hf-repo MaziyarPanahi/gemma-3-1b-it-GGUF --hf-file gemma-3-1b-it.Q5_K_M.gguf # llm available on localhost:8080\n"})}),"\n",(0,s.jsx)(i.h2,{id:"repository",children:"Repository"}),"\n",(0,s.jsx)(i.p,{children:"Please clone the repository."}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"git clone https://github.com/Wyliodrin/edge-ai-chat-with-llm.git\ncd edge-ai-chat-with-llm\n"})}),"\n",(0,s.jsx)(i.h2,{id:"workshop",children:"Workshop"}),"\n",(0,s.jsxs)(i.p,{children:["You will be working inside the ",(0,s.jsx)(i.code,{children:"workshop.rs"})," file. The full implementation is available in the ",(0,s.jsx)(i.code,{children:"full_demo.rs"})," file, in case you get stuck.\nIn order to run the workshop, execute:"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"RUST_LOG=info cargo run --bin workshop\n"})})]})}function c(e={}){const{wrapper:i}={...(0,n.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,i,t)=>{t.d(i,{R:()=>r,x:()=>o});var l=t(6540);const s={},n=l.createContext(s);function r(e){const i=l.useContext(n);return l.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),l.createElement(n.Provider,{value:i},e.children)}}}]);